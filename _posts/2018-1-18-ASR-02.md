---
layout: post
title: Automatic Speech Recognition - Speech Signal Analysis
date: 2018-1-18
categories: blog
header-mask:  0.3
catalog: true
tags: [ASR,NLP]
description: Speech Signal Analysis for ASR。
---  

## Overview 
- [Features for ASR] (#Acoustic Features for ASR) 
- Spectral analysis  
- Cepstral analysis  
- Standard features for ASR: FBANK, MFCCs and PLP analysis  
- Dynamic features  

## Speech production model  <span id="jump">跳转到的地方</span>
![loading](https://raw.githubusercontent.com/zhiyou720/zhiyou720.github.io/master/img/ASR/ASR-02-01.png)

## A/D conversion — Sampling  
Convert analogue signals(模拟信号) in digital form  
![loading](https://raw.githubusercontent.com/zhiyou720/zhiyou720.github.io/master/img/ASR/ASR-02-02.png)
Things to know:  
- Sampling Frequency ($$F_s = 1/T_s$$)
![loading](https://raw.githubusercontent.com/zhiyou720/zhiyou720.github.io/master/img/ASR/ASR-02-03.png)  
- Analogue low-pass filtering to avoid ’aliasing’(混淆)  
NB: the cut-off frequency should be less than the *Nyquist frequency* ($$= F_s/2$$)  

## Acoustic Features for ASR
![loading](https://raw.githubusercontent.com/zhiyou720/zhiyou720.github.io/master/img/ASR/ASR-02-04.png)  
Speech signal analysis to produce a sequence of acoustic feature vectors.  

### Desirable characteristics of acoustic features used for ASR:  
- Features should contain sufficient information to distinguish between phones.  
  - good time resolution (10ms)  
  - good frequency resolution (20 ∼ 40 channels)  
- Be separated from $$F_0$$ and its harmonics(谐波)  
- Be robust against speaker variation(变异)
- Be robust against noise or channel distortions(扭曲)
- Have good “pattern recognition characteristics”
  - low feature dimension
  - features are independent of each other (NB: this applies to GMMs, but not required for NN-based systems)  
  
## MFCC-based front end for ASR
![loading](https://raw.githubusercontent.com/zhiyou720/zhiyou720.github.io/master/img/ASR/ASR-02-05.png)  

## Pre-emphasis and spectral tilt(频谱倾斜)
- Pre-emphasis increases the magnitude(量级) of higher frequencies in the speech signal compared with lower frequencies.
- *Spectral Tilt* 
  - The speech signal has more energy at low frequencies (for voiced speech).
  - This is due to the glottal(声门发声的) source (see the figure)
- Pre-emphasis (first-order) filter boosts higher frequencies:
  - $$x'[t_d]=x[t_d]-\alpha x [t_d-1] \quad \quad 0.95<\alpha<0.99$$  
  
### Examlpe
![loading](https://raw.githubusercontent.com/zhiyou720/zhiyou720.github.io/master/img/ASR/ASR-02-06.png)  

## Windowing  
- The speech signal is constantly changing (non-stationary(静止)).  
- Signal processing algorithms usually assume that the signal is stationary  
- Piecewise(分段) stationarity: model speech signal as a sequence of frames (each assumed to be stationary)
- *Windowing*: multiply the full waveform $$s[n]$$ by a window $$w[n]$$ (in time domain):    
<center>$$x[n] = w[n]s[n] \quad\quad (x_t[n] = w[n]x'[t_d+n])$$</center>  
- Simply cutting out a short segment(分段) (frame) from $$s[n]$$ is a rectangular(矩形) window — causes discontinuities at the edges of the segment.  
- Instead, a tapered(锥形的) window is usually used.  
  - e.g. *Hamming* ($$\alpha$$ = 0.46164) or *Hanning* ($$\alpha$$ = 0.5) window
<center>$$w[n] = (1−\alpha) − \alpha cos(\frac{2\pi n}{L-1}) \quad\quad L : window\quad width$$</center>

### Windowing and spectral analysis(频谱分析)
- Window the signal $$x‘[t_d]$$ into frames $$x_t[n]$$ and apply Fourier Transform(傅立叶变换) to each segment.   
  - Short frame width: wide-band, high time resolution(分辨率), low frequency resolution
  - Long frame width: narrow-band, low time resolution, high frequency resolution
- For ASR:
  - frame width ∼ 25ms
  - frame shift ∼ 10ms
![loading](https://raw.githubusercontent.com/zhiyou720/zhiyou720.github.io/master/img/ASR/ASR-02-07.png)

## Effect of windowing 

### time domain
![loading](https://raw.githubusercontent.com/zhiyou720/zhiyou720.github.io/master/img/ASR/ASR-02-08.png)  

### frequency domain
![loading](https://raw.githubusercontent.com/zhiyou720/zhiyou720.github.io/master/img/ASR/ASR-02-09.png)
![loading](https://raw.githubusercontent.com/zhiyou720/zhiyou720.github.io/master/img/ASR/ASR-02-10.png)  

## Discrete Fourier Transform (DFT)(离散傅立叶变换)
- Purpose: extracts spectral information from a windowed signal (i.e. how much energy at each frequency band)
- Input: windowed signal $$x[0], . . . , x[L−1]$$ (time domain)
- Output: a complex number $$X[k]$$ for each of N frequency bands representing magnitude and phase(相位) for the $$k_th$$ frequency component (frequency domain)
- Discrete Fourier Transform (DFT):  
<center>$$ X[k] =\sum^{N-1}_{n=0} x[n] exp(-j\frac{2\pi}{N} kn)$$</center>  
<p align="right">NB: $$exp(j\theta) = e^{j\theta} = cos(\theta) + j sin(\theta)$$</p>  
- Fast Fourier Transform (FFT)(快速傅立叶变换) — efficient algorithm for computing DFT when $$N$$ is a power of 2, and $$N \geq L$$  

## Wide-band and narrow-band spectrograms(声图谱)
![loading](https://raw.githubusercontent.com/zhiyou720/zhiyou720.github.io/master/img/ASR/ASR-02-11.png)    

## Short-time spectral analysis
![loading](https://raw.githubusercontent.com/zhiyou720/zhiyou720.github.io/master/img/ASR/ASR-02-12.png)    

## DFT Spectrum
![loading](https://raw.githubusercontent.com/zhiyou720/zhiyou720.github.io/master/img/ASR/ASR-02-13.png)    
25ms Hamming window of vowel(元音) /iy/ and its spectrum computed by DFT  

## DFT Spectrum Features for ASR  
- Equally-spaced frequency bands — but human hearing less sensitive at higher frequencies (above ∼ 1000Hz)  
- The estimated power spectrum contains harmonics(谐波) of $$F_0$$, which makes it difficult to estimate the envelope of the spectrum  
![loading](https://raw.githubusercontent.com/zhiyou720/zhiyou720.github.io/master/img/ASR/ASR-02-14.png)
- Frequency bins of STFT are highly correlated each other, i.e. power spectrum representation is highly redundant(冗余)  

## Human hearing  

|Physical quality | Perceptual quality |  
|---------------: |------------------: |  
|        Intensity| Loudness           |  
|Fundamental frequency | Pitch  |
|Spectral shape | Timbre | 
|Onset/offset time | Timing|
|Phase difference in binaural hearing |  Location|  

### Technical terms  

- equal-loudness contours(类似等高线的描绘声音的线)
- masking(掩饰)
- auditory filters (critical-band filters)(听觉过滤)
- critical bandwidth(临界带宽)

#### Equal loudness contour
![loading](https://raw.githubusercontent.com/zhiyou720/zhiyou720.github.io/master/img/ASR/ASR-02-15.png)  

## Nonlinear frequency scaling(非线性频率尺度变换)  

Human hearing is less sensitive to higher frequencies — thus human perception of frequency is nonlinear

- Mel scale:  
$$ M(f) = 1127 ln(1 + \frac{f}{700})$$  
![loading](https://raw.githubusercontent.com/zhiyou720/zhiyou720.github.io/master/img/ASR/ASR-02-16.png)  

- Bark scale:   
$$ b(f ) = 13 arctan(0.00076f ) + 3.5 arctan((\frac{f}{7500})^2)$$  
![loading](https://raw.githubusercontent.com/zhiyou720/zhiyou720.github.io/master/img/ASR/ASR-02-17.png)    

## Mel-Filter Bank(梅尔频谱)

- Apply a mel-scale filter bank to DFT power spectrum to obtain mel-scale power spectrum  
- Each filter collects energy from a number of frequency bands in the DFT  
- Linearly spaced < 1000 Hz, logarithmically spaced > 1000 Hz
p18

$$|Y_t[m]| =\sum^{N}_ {k=1}W_m[k] |X_t[k]|$$  

where: 
  - $$k$$ : DFT bin number ($$1, . . . , N$$)  
  - $$m$$ : mel-filter bank number ($$1, . . . , M$$)  
- How many number of mel-filter channels?
- $$\approx$$ 20 for GMM-HMM based ASR  
- 20 ∼ 40 for DNN (+HMM) based ASR  

### Log Mel Power Spectrum

- Compute the log magnitude squared of each mel-filter bankoutput:  $$log\|Y[m]\| ^2$$
  - Taking the log compresses the dynamic range
  - Human sensitivity to signal energy is logarithmic — i.e. humans are less sensitive to small changes in energy at high energy than small changes at low energy
  - Log makes features less variable to acoustic coupling variations(声学耦合差异)
  - Removes phase(相位) information — not important for speech recognition (not everyone agrees with this)  
- Aka “log mel-filter bank outputs” or “FBANK features”, which are widely used in recent DNN-HMM based ASR systems

## Cepstral Analysis(对数倒谱分析)

- Source-Filter model of speech production  
  - Source: Vocal cord vibrations(声带震动) create a glottal(声门的) source waveform  
  - Filter: Source waveform is passed through the vocal tract(声道): position of tongue, jaw(下巴), etc. give it a particular shape and hence a particular filtering characteristic  
- Source characteristics ($$F_0$$, dynamics of glottal pulse(动态的声门的脉冲)) do not help to discriminate between phones  
- The filter specifies the position of the articulators(判定发声器官的位置)  
- ... and hence is directly related to phone discrimination(音素辨别)  
- Cepstral analysis enables us to separate source and filter  

p19

### The Cepstrum(倒频谱)

- Cepstrum obtained by applying inverse DFT to log magnitude spectrum (may be mel-scaled)
- Cepstrum is time-domain (we talk about quefrency)
- Inverse DFT:

<center>$$x[n] = \frac{1}{N}\sum^{N-1}_ {k=0} X[k] exp(j\frac{2\pi}{N}nk)$$</center>   

- Since log power spectrum is real and symmetric(对称的) the inverse DFT is equivalent to a discrete(离散的) cosine transform (DCT):  

<center>$$y_t[n] =\sum^{M-1}_{m=0}log(|Y_t[m]|) cos (n(m+0.5) \frac{\pi}{M})\quad , n = 0, . . . , J$$</center>

## MFCCs(Mel-frequency cepstral coefficients)(梅尔频率倒谱系数)

- Smoothed spectrum(频谱光滑化): transform to cepstral(倒谱) domain, truncate(截短), transform back to spectral(频谱) domain  
- MFCCs: use the cepstral coefficients directly  
  - Widely used as acoustic(声学) features in HMM-based ASR   
  - First 12 MFCCs are often used as the feature vector (removes $$F_0$$ information)  
  - Less correlated(关联) than spectral features — easier to model than spectral features
  - Very compact representation — 12 features describe a 20ms frame(询框) of data  
  - For standard HMM-based systems, MFCCs result in better ASR performance than filter bank or spectrogram(声谱图) features  
  - MFCCs are not robust against noise

## PLP — Perceptual Linear Prediction（感知线性预测）

p20

- PLP (Hermansky, JASA 1990)  
- Uses equal loudness pre-emphasis(预加权) and cube-root compression(立方根压缩) (motivated by perceptual(感知) results) rather than log compression  
- Uses linear predictive auto-regressive modelling(自动回归的线性预测模型) to obtain cepstral coefficients  
- Compared with MFCCs, PLP has been shown to lead to  
  - slightly better ASR accuracy  
  - slightly better noise robustness  

## Dynamic features

- Speech is not constant frame-to-frame, so we can add features to do with how the cepstral coefficients change over time  
- $$\Delta*$$, $$\Delta^2 * $$ are delta features (dynamic features / time derivatives(导数))  
- Simple calculation of delta features $$d(t)$$ at time $$t$$ for cepstral feature $$c(t)$$ (e.g. $$y_t [j]$$):  
$$d(t)=\frac{c(t+1)-c(t-1)}{2}$$

- More sophisticated approach estimates the temporal derivative(更复杂的方法估计时间导数) by using regression to estimate the slope(斜率) (typically using 4 frames each side)  
- “Standard” ASR features (for GMM-based systems) are 39 dimensions:
  - 12 MFCCs, and energy  
  - 12 $$\Delta$$MFCCs, $$\Delta$$energy  
  - 12 $$\Delta^2$$2MFCCs, $$\Delta^2$$ energy

### Estimating dynamic features

p21

### Feature Transforms

- Orthogonal transformation(正交变换) (orthogonal bases正交基)  
  - DCT (discrete cosine transform离散余弦变换)  
  - PCA (principal component analysis主成分分析)  
- Transformation based on the bases that maximises the separability between classes.  
  - LDA (linear discriminant analysis线性判别) / Fisher’s linear discriminant(Fisher线性判别)  
  - HLDA (heteroscedastic linear discriminant analysis异方差的线性判别分析)  

### Feature Normalisation  

- Basic Idea: Transform the features to reduce mismatch between training and test  
- Cepstral Mean Normalisation倒频谱均值一般化 (CMN): subtract the average feature value from each feature, so each feature has a mean value of 0. makes features robust to some linear filtering of the signal (channel variation)  
- Cepstral Variance Normalisation倒频谱方差一般化 (CVN): Divide feature vector by standard deviation(标准差) of feature vectors, so each feature vector element has a variance of 1  
- Cepstral mean and variance normalisation, CMN/CVN:  
$$ \hat{y}_ t [j]= \frac{y_t [j]-\mu(y[j])}{\sigma(y[j])}  

- Compute mean and variance statistics over longest available segments(最长可用段) with the same speaker/channel  
- Real time normalisation: compute a moving average  







