---
layout: post
title: Machine Translation:Introduction to Neural Networks
date: 2018-1-15
categories: blog
catalog: true
tags: [MT,NLP,ML]
description: 
---  

神经网络是一种机器学习方法，比起其他机器学期方法，它有不同的优点。

### 线性模型
线性模型是统计机器翻译的核心。一个句子$$x$$的潜在翻译由一个特征集合$$h_i(x)$$表示。每一个特征用参数$$\lambda_i$$加权，并得到总体的分数:

$$score(\lambda,x)=\sum_j \lambda_j h_j (x)$$

![](https://raw.githubusercontent.com/zhiyou720/zhiyou720.github.io/master/img/MT/MT-01-01.png "图一")
图一：一个线性模型作为一个网络的图形说明:特征值是输入节点，箭头是权重，而得分是一个输出节点。

最重要的是，我们使用线性模型来组合机器翻译系统的不同组成部分，如语言模型、短语翻译模型、重排序模型，以及句子属性，比如长度等，或短语翻译之间的累积跳跃距离。训练方法为每个特征$$h_i(x)$$赋予一个权重值$$\lambda_i$$，这与它们在提高翻译水平上的重要性有关。在静态的机器翻译中，这称为调优(tuning)。

然而，线性模型不允许我们在特性之间定义更复杂的关系。对于短句来说，翻译模型比语言模型更重要，或者说，平均短语翻译概率高于0.1同样是合理的，但比0.1低的都是是非常糟糕的。第一个假设的例子暗示了特性之间的依赖关系，第二个例子暗示了特征值之间的非线性关系且它对最终分数的影响。线性模型无法处理这些情况。

对于线性模型使用的一个常见的反例是XOR(异或门），布尔运算符$$\oplus$$(0$$\oplus$$0=0, 1$$\oplus$$0=1, 0$$\oplus$$1=1, 1$$\oplus$$1=0)。对于线性模型，具有两个特征(输入)，不可能出现在所有情况下都给出正确输出的权重。线性模型假设所有的实例为特征空间中的点，它们是线性可分的。这不是XOR的情况，也可能不是我们在机器翻译中使用的特征类型。

### 多重网络层

神经网络以两种重要的方式对线性模型进行了修改。第一个是使用多个层。不再直接从输入值计算输出值，而是引入了一个隐藏层。之所以被称为“隐藏”，因为我们可以在训练实例中观察输入和输出，而不是连接它们的机制 -- 这一概念的使用与HMMs模型的含义相似。

图二

如图二，该网络由两个步骤处理。首先，计算加权输入节点的线性组合，以产生每个隐藏的节点值。然后计算加权隐藏节点的线性组合，以产生每个输出节点值。

在这一点上，让我们从神经网络文献中引入数学符号。一个由隐藏层组成的神经网络由：

- 一个带有值的输入节点的向量 $$\vec{x} = (x_1, x_2, x_3, ...x_n)^T$$  
- 一个带有值的隐藏节点的向量 $$\vec{h} = (h_1, h_2, h_3, ...h_m)^T$$
- 一个具有值的输出节点向量 $$\vec{y} = (y_1, y_2, y_3, ...y_l)^T$$
- 将输入节点与隐藏节点连接起来的权重矩阵 $$W = \{ \omega_{ij} \} $$
- 将输出节点与隐藏节点连接起来的权重矩阵 $$U = \{ \mu_{ij} \}$$

在神经网络中，有一个隐藏层的计算:

$$h_j = \sum_i x_i w_{ji} \quad \quad \quad y_k = \sum_j h_j \mu_{kj}$$

注意，我们可能会有多个输出节点$$y_k$$的可能性，尽管图片只显示了一个。

### 非线性

如果我们仔细考虑添加一个隐藏层，我们就会意识到到目前为止我们还没有获得任何对输入/输出关系的模型。通过将权重相乘，我们可以很容易地去掉隐藏层：

$$y_k=\sum_j h_j \mu_{kj} = \sum_j \sum_i x_i \omega_{ji} \mu_{kj} =  \sum_i x_i (\sum_j \mu_{kj} \omega_{ji})$$


因此，神经网络的一个显著要素是使用非线性激活函数。在计算加权特征值$$s_j = \sum_i x_i w_{ji}$$的线性组合后，只有在应用了这样的函数$$h_j = f (s_j)$$后，才能得到一个节点的值。









