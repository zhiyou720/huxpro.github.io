---
layout: post
title: Automatic Speech Recognition - Hidden Markov Models(HMMs) and Gaussian Mixture Models(GMMs)
date: 2018-1-25
categories: blog
catalog: true
tags: [ASR,NLP]
description: HMMs,GMMs
---  

# Overview

- Key models and algorithms for HMM acoustic models
- Gaussians
- GMMs: Gaussian mixture models
- HMMs: Hidden Markov models
- HMM algorithms
  - Likelihood computation (forward algorithm)
  - Most probable state sequence (Viterbi algorithm)
  - Estimating the parameters (EM algorithm)

# Fundamental Equation of Statistical Speech Recognition

If $$X$$ is the sequence of acoustic feature vectors (observations) and $$W$$ denotes a word sequence, the most likely word sequence $$W*$$ is given by:

$$W* = arg \max_W P(W|X)$$

Applying Bayes’ Theorem:  

$$P(W|X) = \frac{p(X|W) P(W)}{p(X)} \propto p(X|W) P(W)$$ 

$$W* = arg \max_W p(X|W) P(W)$$  

where $$p(X\mid W)$$ is acoustic model and $$P(W)$$ is language model.NB: $$X$$ is used hereafter to denote the output feature vectors from the signal analysis module rather than DFT spectrum.  

# Acoustic Modelling

p1

# Hierarchical modelling of speech

p2

# Calculation of $$p(X\mid W)$$

p3

# How to calculate $$p(X_1\mid /s/)$$?

Assume $$x_1, x_2, · · · , x_{T_1}$$ corresponds to phoneme /s/, the conditional probability that we observe the sequence is:

$$p(X_1|/s/) = p(x_1, · · · , x_{T_1}|/s/), x_i = (x_{1i}, · · · , x_{Di})^t \in \mathcal{R}^d$$

We know that HMM can be employed to calculate this. (Viterbi algorithm, Forward / Backward algorithm)  
To grasp the idea of probability calculation, let’s consider an extremely simple case where the length of input sequence is just one ($$T_1 = 1$$), and the dimensionality of $$x$$ is one ($$d = 1$$), so that we don’t need HMM:

$$p(X_1|/s/) → p(x_1|/s/)$$

$$p(x|/s/)$$ : conditional probability (conditional probability density function ($$pdf$$) of $$x$$)  
- A Gaussian / normal distribution function could be employed for this:

$$P(x\mid /s/)= \frac{1}{\sqrt{2 \pi \sigma^{2}_{s}}}e^{-\frac{(x-\mu_s)^2}{2\sigma^{2}_{s}}}$$

- The function has only two parameters, $$\mu_s$$ and $$\sigma^{2}_ {s}$$  
- Given a set of training samples {$$x_1, · · · , x_N$$}, we can estimate $$\mu_s$$ and $$\sigma^{2}_ {s}$$:

$$\hat{\mu}_ s = \frac{1}{N} \sum^{N}_ {i=1} x_i\quad\quad\quad \hat{\sigma}^{2}_ {s} = \frac{1}{N}\sum^{N}_ {i=1}(x_i-\hat{\mu}_s)^2$$

For a general case where a phone lasts more than one frame, we need to employ HMM.

# Acoustic Model: Continuous Density HMM

p4 p5

Parameters $$\lambda$$:  
- Transition probabilities: $$a_kj$$ = P(S =j \mid S =k)$$  
- Output probability density function: $$b_j (x) = p(x \mid S =j)$$  
NB: Some textbooks use $$Q$$ or $$q$$ to denote the state variable $$S$$. $$x$$ corresponds to $$o_t$$ in Lecture slides 02.

# HMM Assumptions

p6

1. Markov process: The probability of a state depends only on the previous state:

$$ P(S(t)|S(t−1), S(t−2), . . . , S(1)) = P(S(t)|S(t−1))$$  
A state is conditionally independent of all other states given the previous
state

2. Observation independence: The output observation $$x(t)$$ depends only on the state that produced the observation:

$$p(x(t)|S(t), S(t−1), . . . , S(1), x(t−1), . . . , x(1)) = p(x(t)|S(t))$$  
An acoustic observation x is conditionally independent of all other observations given the state that generated it

# Output distribution

p7 

- Single multivariate Gaussian with mean $$\mu_j , covariance matrix $$\sum_j$$:

$$b_j(x) = p(x\mid S =j) = \mathcal{N} (x; \mu_j , \sum_j)$$

- M-component Gaussian mixture model:

$$b_j(x) = p(x|S =j) = \sum^{M}_ {m=1} c_{jm} \mathcal{N} (x; \mu_{jm}, \sum_{jm})$$

- Neural nerwork:  

$$b_j (x) ~ P(S=j\mid x)/P(S=j)$$  
BN:NN outputs posterior probabilities

# Background: cdf  
 
- Consider a real valued random variable $$X$$  
  - Cumulative distribution function (cdf) $$F(x)$$ for $$X$$:
  
  $$F(x)=P(X\leq x)$$
  
  - To obtain the probability of falling in an interval we can do the following:

$$P(a<X\leq b)= P(X\leq b)- P(X\leq a)= F(b)-F(a)$$

# Background: pdf

- The rate of change of the cdf gives us the probability density function (pdf), $$p(x)$$:

$$p(x) = \frac{d}{dx} F(x) = F'(x)$$
$$F(x) = \int^{x}{−\infty}p(x)dx$$

- $$p(x)$$ is not the probability that $$X$$ has value $$x$$. But the pdf is proportional to the probability that $$X$$ lies in a small interval centred on $$x$$.  
- Notation: $$p$$ for pdf, $$P$$ for probability










