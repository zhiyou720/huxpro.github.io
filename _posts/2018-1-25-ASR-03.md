---
layout: post
title: Automatic Speech Recognition - Hidden Markov Models(HMMs) and Gaussian Mixture Models(GMMs)
date: 2018-1-25
categories: blog
catalog: true
tags: [ASR,NLP]
description: HMMs,GMMs
---  

# Overview

- Key models and algorithms for HMM acoustic models
- Gaussians
- GMMs: Gaussian mixture models
- HMMs: Hidden Markov models
- HMM algorithms
  - Likelihood computation (forward algorithm)
  - Most probable state sequence (Viterbi algorithm)
  - Estimating the parameters (EM algorithm)

# Fundamental Equation of Statistical Speech Recognition

If $$X$$ is the sequence of acoustic feature vectors (observations) and $$W$$ denotes a word sequence, the most likely word sequence $$W*$$ is given by:

$$W* = arg \max_W P(W|X)$$

Applying Bayes’ Theorem:  

$$P(W|X) = \frac{p(X|W) P(W)}{p(X)} \propto p(X|W) P(W)$$ 

$$W* = arg \max_W p(X|W) P(W)$$  

where $$p(X\mid W)$$ is acoustic model and $$P(W)$$ is language model.NB: $$X$$ is used hereafter to denote the output feature vectors from the signal analysis module rather than DFT spectrum.  

# Acoustic Modelling

![](https://raw.githubusercontent.com/zhiyou720/zhiyou720.github.io/master/img/ASR/ASR-03-01.png)

# Hierarchical modelling of speech

![](https://raw.githubusercontent.com/zhiyou720/zhiyou720.github.io/master/img/ASR/ASR-03-02.png)

# Calculation of $$p(X\mid W)$$

![](https://raw.githubusercontent.com/zhiyou720/zhiyou720.github.io/master/img/ASR/ASR-03-03.png)

# How to calculate $$p(X_1\mid /s/)$$?

Assume $$x_1, x_2, · · · , x_{T_1}$$ corresponds to phoneme /s/, the conditional probability that we observe the sequence is:

$$p(X_1|/s/) = p(x_1, · · · , x_{T_1}|/s/), x_i = (x_{1i}, · · · , x_{Di})^t \in \mathcal{R}^d$$

We know that HMM can be employed to calculate this. (Viterbi algorithm, Forward / Backward algorithm)  
To grasp the idea of probability calculation, let’s consider an extremely simple case where the length of input sequence is just one ($$T_1 = 1$$), and the dimensionality of $$x$$ is one ($$d = 1$$), so that we don’t need HMM:

$$p(X_1|/s/) → p(x_1|/s/)$$

$$p(x|/s/)$$ : conditional probability (conditional probability density function ($$pdf$$) of $$x$$)  
- A Gaussian / normal distribution function could be employed for this:

$$P(x\mid /s/)= \frac{1}{\sqrt{2 \pi \sigma^{2}_{s}}}e^{-\frac{(x-\mu_s)^2}{2\sigma^{2}_{s}}}$$

- The function has only two parameters, $$\mu_s$$ and $$\sigma^{2}_ {s}$$  
- Given a set of training samples {$$x_1, · · · , x_N$$}, we can estimate $$\mu_s$$ and $$\sigma^{2}_ {s}$$:

$$\hat{\mu}_ s = \frac{1}{N} \sum^{N}_ {i=1} x_i\quad\quad\quad \hat{\sigma}^{2}_ {s} = \frac{1}{N}\sum^{N}_ {i=1}(x_i-\hat{\mu}_s)^2$$

For a general case where a phone lasts more than one frame, we need to employ HMM.

# Acoustic Model: Continuous Density HMM

![](https://raw.githubusercontent.com/zhiyou720/zhiyou720.github.io/master/img/ASR/ASR-03-04.png) 
![](https://raw.githubusercontent.com/zhiyou720/zhiyou720.github.io/master/img/ASR/ASR-03-05.png)

Parameters $$\lambda$$:  
- Transition probabilities: $$a_kj = P(S =j \mid S =k)$$  
- Output probability density function: $$b_j (x) = p(x \mid S =j)$$  
NB: Some textbooks use $$Q$$ or $$q$$ to denote the state variable $$S$$. $$x$$ corresponds to $$o_t$$ in Lecture slides 02.

# HMM Assumptions

![](https://raw.githubusercontent.com/zhiyou720/zhiyou720.github.io/master/img/ASR/ASR-03-06.png)

1. Markov process: The probability of a state depends only on the previous state:

   $$ P(S(t)|S(t−1), S(t−2), . . . , S(1)) = P(S(t)|S(t−1))$$  

   A state is conditionally independent of all other states given the previous state

2. Observation independence: The output observation $$x(t)$$ depends only on the state that produced the observation:

   $$p(x(t)|S(t), S(t−1), . . . , S(1), x(t−1), . . . , x(1)) = p(x(t)|S(t))$$  

   An acoustic observation x is conditionally independent of all other observations given the state that generated it

# Output distribution

![](https://raw.githubusercontent.com/zhiyou720/zhiyou720.github.io/master/img/ASR/ASR-03-07.png) 

- Single multivariate Gaussian with mean $$\mu_j$$ , covariance matrix $$\sum_j$$:

$$b_j(x) = p(x\mid S =j) = \mathcal{N} (x; \mu_j , \sum_j)$$

- M-component Gaussian mixture model:

$$b_j(x) = p(x|S =j) = \sum^{M}_ {m=1} c_{jm} \mathcal{N} (x; \mu_{jm}, \sum_{jm})$$

- Neural nerwork:  

$$b_j (x) ~ P(S=j\mid x)/P(S=j)$$  

BN:NN outputs posterior probabilities

# Background: cdf  
 
- Consider a real valued random variable $$X$$  
  - Cumulative distribution function (cdf) $$F(x)$$ for $$X$$:
  
  $$F(x)=P(X\leq x)$$
  
  - To obtain the probability of falling in an interval we can do the following:

$$P(a<X\leq b)= P(X\leq b)- P(X\leq a)= F(b)-F(a)$$

# Background: pdf

- The rate of change of the cdf gives us the probability density function (pdf), $$p(x)$$:

$$p(x) = \frac{d}{dx} F(x) = F'(x)$$

$$F(x) = \int^{x}_ {−\infty}p(x)dx$$

- $$p(x)$$ is not the probability that $$X$$ has value $$x$$. But the pdf is proportional to the probability that $$X$$ lies in a small interval centred on $$x$$.  
- Notation: $$p$$ for pdf, $$P$$ for probability

# The Gaussian distribution (univariate单变量的)

- The Gaussian (or Normal) distribution is the most common (and easily analysed) continuous distribution  
- It is also a reasonable model in many situations (the famous “bell curve”)
- If a (scalar) variable has a Gaussian distribution, then it has a probability density function with this form:

$$ p(x \mid \mu, \sigma^2) = \mathcal{N} (x; \mu, \sigma^2 ) = \frac{1}{\sqrt{2\pi \sigma^2}} exp (\frac{−(x − µ)^2}{2\sigma^2})$$


- The Gaussian is described by two parameters:
  - the mean \mu (location) 
  - the variance \sigma^2(dispersion分布)


# Plot of Gaussian distribution 

- Gaussians have the same shape, with the location controlled by the mean, and the spread(幅度) controlled by the variance.
- One-dimensional Gaussian with zero mean and unit variance($$\mu = 0, \sigma^2 = 1$$):

![](https://raw.githubusercontent.com/zhiyou720/zhiyou720.github.io/master/img/ASR/ASR-03-08.png)

# Properties of the Gaussian distribution

$$ \mathcal{N} (x; \mu, \sigma^2 ) = \frac{1}{\sqrt{2\pi \sigma^2}} exp (\frac{−(x − µ)^2}{2\sigma^2})$$

![](https://raw.githubusercontent.com/zhiyou720/zhiyou720.github.io/master/img/ASR/ASR-03-09.png)

# Parameter estimation  

- Estimate mean and variance parameters of a Gaussian from data $$x_1, x_2, . . . , x_T$$
- Use the following as the estimates:  

$$ \hat{\mu} =\frac{1}{T}\sum^{T}_ {t=1}x_t\quad\quad (mean)$$  

$$ \hat{\sigma}^2 = \frac{1}{T}\sum^{T}{t=1}(x_t − \hat{\mu})^2\quad\quad(variance)$$

# Exercise — maximum likelihood estimation (MLE)

Consider the log likelihood of a set of $$T$$ training data points $${x_1, . . . , x_T }$$ being generated by a Gaussian with mean $$\mu$$ and variance \sigma^2:  

$$L = \ln p({x_1, . . . , x_T}\mid \mu , \sigma^2) = −\frac{1}{2}\sum^{T}_ {t=1}(\frac{(x_t − \mu)^2}{\sigma^2} − \ln \sigma^2 − \ln(2\pi))$$

$$ = −\frac{1}{2\sigma^2}\sum^{T}_ {t=1}(x_t − \mu)^2 - \frac{T}{2}\ln \sigma^2- \frac{T}{2}\ln(2\pi)$$

By maximising the the log likelihood function with respect to $$\mu$$ show that the maximum likelihood estimate for the mean is indeed the sample mean:

$$\mu_{M}L =\frac{1}{T}\sum^{T}_ {t=1}x_t$$

# The multivariate(多变量) Gaussian distribution

- The $$D$$-dimensional vector $$x = (x_1, . . . , x_D)^T$$ follows a multivariate Gaussian (or normal) distribution if it has    a probability density function of the following form:

  $$p(x\mid \mu, \sum) = \frac{1}{(2π)^{D/2}\mid \sum \mid^{\frac{1}{2}}} \exp(−\frac{1}{2}(x − \mu)^T {\sum}^{−1}(x − \mu))$$

  The pdf is parameterised by the mean vector $$\mu = (\mu_1, . . . , \mu_D)^T$$

  and the covariance matrix $$ \sum=
  \left(
  \begin{array}{ccc}
  \sigma_{11} & \cdots & \sigma_{1D}\\
  \vdots & \ddots & \vdots\\
  \sigma_{D1} & \cdots & \sigma_{DD}
  \end{array}
  \right)
  $$

- The 1-dimensional Gaussian is a special case of this pdf  
- The argument to the exponential $$0.5(x − \mu)^T {\sum}^{−1} (x − \mu)$$ is referred to as a quadratic form. 

# Covariance matrix


- The mean vector $$\mu$$ is the expectation of $$x$$:

  $$\mu = E[x]$$

- The covariance matrix $$\sum$$ is the expectation of the deviation of $$x$$ from the mean:

  $$\sum = E[(x − \mu)(x − \mu)^T]$$

- $$\sum$$ is a $$D × D$$ symmetric matrix:

  $$ \sigma_{ij} = E[(x_i − \mu_i)(x_j − \mu_j)] = E[(x_j − \mu_j)(x_i − \mu_i)] = \sigma_{ji}$$
  
- The sign of the covariance helps to determine the relationship between two components:  

  - If $$x_j$$ is large when $$x_i$$ is large, then $$(x_i − \mu_i)(x_j − \mu_j)$$ will tend to be positive;
  - If $$x_j$$ is small when $$x_i$$ is large, then $$(x_i − \mu_i)(x_j − \mu_j)$$ will tend to be negative.
  
  
# Spherical(球面) Gaussian

  ![](https://raw.githubusercontent.com/zhiyou720/zhiyou720.github.io/master/img/ASR/ASR-03-10.png)


  $$\mu = 
  \left(
  \begin{array}{ccc}
  0 \\
  0 
  \end{array}
  \right)
  \quad\quad
  \sum = 
  \left(
  \begin{array}{ccc}
  1 & 0 \\
  0 & 1
  \end{array}
  \right)
  \quad\quad
  \rho_ {12} = 0
  $$
  
  NB: Correlation coefficient $$\rho_ {ij} = \frac{\sigma_{ij}}{\sqrt{\sigma_{ii}\sigma_{jj}}} \quad\quad (−1 \leq \rho_{ij} \leq 1)$$





# Diagonal Covariance(对角协方差) Gaussian

  ![](https://raw.githubusercontent.com/zhiyou720/zhiyou720.github.io/master/img/ASR/ASR-03-11.png)
 
  
  $$\mu = 
  \left(
  \begin{array}{ccc}
  0 \\
  0 
  \end{array}
  \right)
  \quad\quad
  \sum = 
  \left(
  \begin{array}{ccc}
  1 & 0 \\
  0 & 4
  \end{array}
  \right)
  \quad\quad
  \rho_ {12} = 0
  $$
  
  NB: Correlation coefficient $$\rho_ {ij} = \frac{\sigma_{ij}}{\sqrt{\sigma_{ii}\sigma_{jj}}} \quad\quad (−1 \leq \rho_{ij} \leq 1)$$
  
  
# Full covariance Gaussian
  
  ![](https://raw.githubusercontent.com/zhiyou720/zhiyou720.github.io/master/img/ASR/ASR-03-12.png)

  
$$\mu = 
  \left(
  \begin{array}{ccc}
  0 \\
  0 
  \end{array}
  \right)
  \quad\quad
  \sum = 
  \left(
  \begin{array}{ccc}
  1 & -1 \\
  -1 & 4
  \end{array}
  \right)
  \quad\quad
  \rho_ {12} = 0.5
  $$  
  
  NB: Correlation coefficient $$ \rho_ {ij} = \frac{\sigma_{ij}}{\sqrt{\sigma_{ii}\sigma_{jj}}} \quad\quad (−1 \leq \rho_{ij} \leq 1)$$
  
  
# Parameter estimation of a multivariate Gaussian distribution
  
- It is possible to show that the mean vector $$\hat{\mu}$$ and covariance matrix $$\hat{\sum} that maximise the likelihood of the training data are given by:  
  
  $$\hat{\mu} = \frac{1}{T}\sum^{T}_ {t=1}x_t$$
  
  $$\hat{\sum}=\frac{1}{T}\sum^{T}_ {t=1}(x_t-\hat{\mu})(x_t-\hat{\mu})^T$$
  
  where $$x_t = (x_{t1},...,x_{tD})^T$$
  
  NB: $$T$$ denotes either the number of samples or vector transpose depending on context.
  
# Example data  
  
  ![](https://raw.githubusercontent.com/zhiyou720/zhiyou720.github.io/master/img/ASR/ASR-03-13.png)  
  
# Maximum likelihood fit to a Gaussian 
  
  ![](https://raw.githubusercontent.com/zhiyou720/zhiyou720.github.io/master/img/ASR/ASR-03-14.png)
  
# Data in clusters (example 1)
  
  ![](https://raw.githubusercontent.com/zhiyou720/zhiyou720.github.io/master/img/ASR/ASR-03-15.png)
  
# Example 1 fit by a Gaussian
  
  ![](https://raw.githubusercontent.com/zhiyou720/zhiyou720.github.io/master/img/ASR/ASR-03-16.png)

### k-means clustering  
  
  - k-means is an automatic procedure for clustering unlabelled data
  - Requires a prespecified number of clusters
  - Clustering algorithm chooses a set of clusters with the minimum within-cluster variance
  - Guaranteed to converge (eventually)
  - Clustering solution is dependent on the initialisation
  
#### k-means example: data set

  ![](https://raw.githubusercontent.com/zhiyou720/zhiyou720.github.io/master/img/ASR/ASR-03-17.png)
  
#### k-means example: initialisation 
![](https://raw.githubusercontent.com/zhiyou720/zhiyou720.github.io/master/img/ASR/ASR-03-18.png)

## k-means example: iteration 1 (assign points to clusters)
![](https://raw.githubusercontent.com/zhiyou720/zhiyou720.github.io/master/img/ASR/ASR-03-19.png)  
 
## k-means example: iteration 1 (recompute centres) 
![](https://raw.githubusercontent.com/zhiyou720/zhiyou720.github.io/master/img/ASR/ASR-03-20.png)

## k-means example: iteration 2 (assign points to clusters)
![](https://raw.githubusercontent.com/zhiyou720/zhiyou720.github.io/master/img/ASR/ASR-03-21.png)

## k-means example: iteration 2 (recompute centres)
![](https://raw.githubusercontent.com/zhiyou720/zhiyou720.github.io/master/img/ASR/ASR-03-22.png)

## k-means example: iteration 3 (assign points to clusters)
![](https://raw.githubusercontent.com/zhiyou720/zhiyou720.github.io/master/img/ASR/ASR-03-23.png)
  
# Mixture model  

- A more flexible form of density estimation is made up of a linear combination of component densities:
  
  $$p(x) = \sum^{M}_ {m=1} P(m)p(x\mid m)$$
  
- This is called a mixture model or a mixture density.
- $$p(x\mid m)$$ : component densities
- $$P(m)$$ : mixing parameters
- Generative model:
  1.  Choose a mixture component based on $$P(m)$$
  2.  Generate a data point $$x$$ from the chosen component using $$p(x\mid m)$$
  
# Gaussian mixture model  

- The most important mixture model is the Gaussian Mixture Model (GMM), where the component densities are Gaussians
- Consider a GMM, where each component Gaussian $$\mathcal{N} (x; \mu_m, {\sum}_ m)$$ has mean $$\mu_m$$ and a spherical covariance $${\sum}_ m = \sigma^{2}_ {m}I$$

$$p(x) = \sum^{M}_ {m=1}P(m) p(x\mid m) = \sum^{M}_ {m=1}P(m) \mathcal{N} (x; \mu_m, \sigma^{2}_ {m}I)$$

p24

# GMM Parameter estimation when we know which component generated the data

- Define the indicator variable $$z_{mt} = 1$$ if component $$m$$ generated data point $$x_t$$ (and 0 otherwise)
- If $$z_mt$$ wasn’t hidden then we could count the number of observed data points generated by m:

$$N_m =\sum^{T}_ {t=1}z_{mt}$$

- And estimate the mean, variance and mixing parameters as:

  $$\hat{\mu}_ m =\frac{{\sum}_t z_{mt}x_t}{N_m}$$

  $$\hat{\sigma}^{2}_ {m} =\frac{{\sum}_ t z_{mt}\| x_t−\hat{\mu}_ m\|^2}{N_m}$$
  
  $$\hat{P}(m) = \frac{1}{T}\sum_t z_{mt} = \frac{N_m}{T}$$






















