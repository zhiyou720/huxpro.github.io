---
layout: post
title: Automatic Speech Recognition - Hidden Markov Models(HMMs) and Gaussian Mixture Models(GMMs)
date: 2018-1-25
categories: blog
catalog: true
tags: [ASR,NLP]
description: HMMs,GMMs
---  

# Overview

- Key models and algorithms for HMM acoustic models
- Gaussians
- GMMs: Gaussian mixture models
- HMMs: Hidden Markov models
- HMM algorithms
  - Likelihood computation (forward algorithm)
  - Most probable state sequence (Viterbi algorithm)
  - Estimating the parameters (EM algorithm)

# Fundamental Equation of Statistical Speech Recognition

If $$X$$ is the sequence of acoustic feature vectors (observations) and $$W$$ denotes a word sequence, the most likely word sequence $$W*$$ is given by:

$$W* = arg \max_W P(W|X)$$

Applying Bayes’ Theorem:  

$$P(W|X) = \frac{p(X|W) P(W)}{p(X)} \propto p(X|W) P(W)$$ 

$$W* = arg \max_W p(X|W) P(W)$$  

where $$p(X\mid W)$$ is acoustic model and $$P(W)$$ is language model.NB: $$X$$ is used hereafter to denote the output feature vectors from the signal analysis module rather than DFT spectrum.  

# Acoustic Modelling

p1

# Hierarchical modelling of speech

p2

# Calculation of $$p(X\mid W)$$

p3

# How to calculate $$p(X_1\mid /s/)$$?

Assume $$x_1, x_2, · · · , x_{T_1}$$ corresponds to phoneme /s/, the conditional probability that we observe the sequence is:

$$p(X_1|/s/) = p(x_1, · · · , x_{T_1}|/s/), xi = (x_{1i}, · · · , x_{Di})^t \in \mathcal{R}^d$$

















